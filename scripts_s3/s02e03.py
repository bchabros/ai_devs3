from langchain_pinecone import PineconeVectorStorefrom langchain_openai import OpenAIEmbeddings, ChatOpenAIfrom langchain_community.document_loaders import DirectoryLoaderfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.chains import RetrievalQAfrom langchain.prompts import PromptTemplatefrom pinecone import Pineconeimport osfrom dotenv import load_dotenvdef load_documents(directory_path, file_pattern="*.txt"):    """Load documents from specified directory"""    loader = DirectoryLoader(directory_path, glob=file_pattern)    return loader.load()def split_documents(docs, chunk_size=1000, chunk_overlap=200):    """Split documents into smaller chunks"""    text_splitter = RecursiveCharacterTextSplitter(        chunk_size=chunk_size,        chunk_overlap=chunk_overlap    )    return text_splitter.split_documents(docs)def initialize_embeddings(api_key, model="text-embedding-3-small"):    """Initialize OpenAI embeddings"""    return OpenAIEmbeddings(        openai_api_key=api_key,        model=model    )def get_vector_store(index_name, embeddings, refresh=False, documents_path=None):    """    Get or create vector store based on refresh parameter    Args:        index_name (str): Name of the Pinecone index        embeddings: OpenAI embeddings instance        refresh (bool): Whether to refresh the vector store        documents_path (str): Path to documents directory (required if refresh=True)    Returns:        PineconeVectorStore instance    """    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))    try:        index = pc.Index(index_name)        if refresh:            if not documents_path:                raise ValueError("documents_path is required when refresh=True")            try:                # try delete index                index.delete(delete_all=True)                print(f"All vectors deleted from index '{index_name}'")            except Exception as e:                # If deletion fails (e.g., no vectors exist), just proceed with creation                print(f"No existing vectors to delete in index '{index_name}'")            # Load and process new documents            docs = load_documents(documents_path)            split_docs = split_documents(docs)            # Create new vector store            return PineconeVectorStore.from_documents(                split_docs,                embeddings,                index_name=index_name            )        else:            # Check if index has any vectors            stats = index.describe_index_stats()            if stats.total_vector_count == 0:                raise ValueError(f"Index '{index_name}' is empty. Please use refresh=True to populate it.")            # Return existing vector store            return PineconeVectorStore(                index_name=index_name,                embedding=embeddings            )    except Exception as e:        if "Index not found" in str(e):            if refresh and documents_path:                # If index doesn't exist, and we're refreshing, create it with new documents                docs = load_documents(documents_path)                split_docs = split_documents(docs)                return PineconeVectorStore.from_documents(                    split_docs,                    embeddings,                    index_name=index_name                )            else:                raise ValueError(f"Index '{index_name}' does not exist. Please use refresh=True to create it.")        else:            raise edef setup_qa_chain(vector_store, model_name="gpt-4o-mini", temperature=0):    """Set up the QA chain with specified LLM and vector store"""    llm = ChatOpenAI(        model_name=model_name,        temperature=temperature    )    # Create a custom prompt template that includes metadata    prompt_template = """You are an AI assistant helping to analyze reports from a factory.         Use the following pieces of context (including their source files/dates) to answer the question at the end.         If you don't know the answer, just say that you don't know, don't try to make up an answer.        Usually File name has a Date which is mentioned in the context.        Context: {context}        Question: {question}        When referring to dates, always mention them explicitly from the source files.        Answer in the same language as the question.        """    prompt = PromptTemplate(        template=prompt_template,        input_variables=["context", "question"]    )    return RetrievalQA.from_chain_type(        llm=llm,        chain_type="stuff",        retriever=vector_store.as_retriever(),        return_source_documents=True,  # This will return source documents along with the answer        chain_type_kwargs={            "prompt": prompt,        }    )def query_document(qa_chain, query):    """Execute a query against the QA chain"""    result = qa_chain.invoke(query)    # Extract the answer and source documents    answer = result['result']    source_docs = result['source_documents']    # Print the answer    print("\nAnswer:", answer)    # Print the sources used    print("\nSources used:")    for doc in source_docs:        source_file = os.path.basename(doc.metadata['source'])        print(f"- {source_file}")    return resultif __name__ == "__main__":    load_dotenv()    # Configuration    DOCUMENTS_PATH = "/Users/Chabi/Desktop/ai_devs/pliki_z_fabryki/do-not-share"    INDEX_NAME = "ai-devs-s02e03"    REFRESH = False  # Set to True to refresh the vector store    QUERY = "W raporcie, z którego dnia znajduje się wzmianka o kradzieży prototypu broni?"  # Example Query    try:        # Initialize embeddings        embeddings = initialize_embeddings(os.getenv("OPENAI_API_KEY"))        # Get or create vector store based on REFRESH flag        vector_store = get_vector_store(            index_name=INDEX_NAME,            embeddings=embeddings,            refresh=REFRESH,            documents_path=DOCUMENTS_PATH if REFRESH else None        )        # Setup QA chain        qa_chain = setup_qa_chain(vector_store)        # Example query        result = query_document(qa_chain, QUERY)        print(result['result'])    except Exception as e:        print(f"Error: {str(e)}")